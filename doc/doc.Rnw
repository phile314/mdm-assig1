\documentclass[12pt, a4paper, oneside]{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{pdflscape}
% font size could be 10pt (default), 11pt or 12 pt
% paper size coulde be letterpaper (default), legalpaper, executivepaper,
% a4paper, a5paper or b5paper
% side coulde be oneside (default) or twoside 
% columns coulde be onecolumn (default) or twocolumn
% graphics coulde be final (default) or draft 
%
% titlepage coulde be notitlepage (default) or titlepage which 
% makes an extra page for title 
% 
% paper alignment coulde be portrait (default) or landscape 
%
% equations coulde be 
%   default number of the equation on the rigth and equation centered 
%   leqno number on the left and equation centered 
%   fleqn number on the rigth and  equation on the left side
%   
\title{MDM Assignment 1}
\author{Marco Vassena  \\
    4110161 \\
    \and 
    Philipp Hausmann \\
    4003373 \\
    }

\date{\today} 
\begin{document}

<<setup, echo=FALSE, cache=FALSE,include=FALSE>>=
library(knitr)
library(xtable)
opts_chunk$set(echo=FALSE)

source('../src/common.r', chdir=TRUE)
source('../src/functional.r', chdir=TRUE)
source('../src/ctree.r', chdir=TRUE)

@
<<init, echo=FALSE, cache=TRUE, include=FALSE>>=
do_eval_perf <- FALSE
do_eval_par <- TRUE
nrep <- 1


dat_perf <- read_data('../data/spambase.data', 0.0)
dat_pars <- read_data('../data/spambase.data', 0.7)

@

\maketitle

\tableofcontents


\section{Problem description}
The goal of this assignment is to implement and evaluate a tree classification algorithm.
In addition to that, we will also discuss two different implementation styles
and their performance implications.

\section{Implementations}
We created two implementations of the algorithm. The first one is idiomatic R code,
whereas the second one is inspired by what a functional implementation looks like.
Both algorithms produce the same results, they may only differ performance-wise.

\subsection{Basic Functions}
The file \texttt{common.r} contains utility functions common to both the implementations. 
In this section we will point out only the peculiarities about some of those functions.

	\paragraph{\texttt{best.split.on}} 
	This function computes the best possible split, for a given vector of 
	attributes. Note that this function supports both numerical and binary 
	(encoded as 0 and 1) attributes.
	Since the returned split must  also satisfy the \texttt{minleaf} constraint we 
	did not adopt the optimization of considering exclusively segment borders, 
	as they are not compatible.

	\paragraph{\texttt{majority\_vote}}
	This function computes the majority class prediction for a given vector 
	containing binary labels (0 and 1). Ties are broken at random.

\subsection{Imperative/R Style}
The classification tree has been implemented as a non-empty data frame with the following columns: \texttt{left}, \texttt{right}, \texttt{label}, \texttt{split}, \texttt{splitCol}. Each row represents either a leaf or an internal node and the first row is the root of the tree.
The \texttt{left} and \texttt{right} field of a node row contains the index of the row of the same data frame at which the correspondent left and right child is to be found.
The \texttt{split} and \texttt{splitCol} are used by the \texttt{classify} procedure. The first is the threshold value
and the second is the column number of the attribute referred by the first.
The field \texttt{label} is set to \texttt{NA} for node rows.
Leaves nodes fields are all set to \texttt{NA} except for \texttt{label} which contains the predicted class label (0 or 1), computed using majority vote in the \texttt{grow} procedure.

\subsection{Functional Style}
The functional style version encodes the tree by simulating objects in R. We
would like to note that R is not perfectly suited for this approach, but it
should give some indications how such an approach compares in term of
performance to a more procedural/imperative approach. The source code can
be found in the file \texttt{functional.r}.

The implementation basically uses recursion to construct the tree, which happens
in the \texttt{tree.functional.grow} function. A nice side effect of the functional
approach is, that the tree is never modified and the recursive approach could easily
be parallelized (in general, not necessarily in R).

\section{Results}
\subsection{Data Set description}
We have tested our implementation with the data set 
\href{https://archive.ics.uci.edu/ml/datasets/Spambase}{SPAM E-mail Database} \cite{SPAM}.
The data set contains only continuos and binary attributes and no missing values, 
thus it's compatible with our implementation.
We have chosen this data set, because it has a great number of attributes (57)
thus it can be conviniently analyzed by classification trees, as they automatically
select relevant attributes.
The attributes include percentage of specific words or characters and statistics 
(average, longest sequence, total length) about sequences of capital letters in the email.
The data set include also a binary attribute that denotes whether an email is spam or not.
For further information about the data set see \cite{SPAM}.
We have trained a tree with different settings of the \texttt{nmin} and \texttt{minleaf} 
parameters using 70\% of our data set (randomly sampled), and we used the remaining 
30\% for testing. The results are listed in \ref{subsec:params}.

\subsection{Performance comparison}
\begin{figure}[!ht]
<<perf-comp-comp,cache=TRUE,echo=FALSE>>=
if(do_eval_perf) {
    ti_fu_gr <- system.time(replicate(nrep, tree.functional.grow(dat_perf$trxs, dat_perf$trys, 5, 5)))["elapsed"]
    ti_im_gr <- system.time(replicate(nrep, tree.grow(dat_perf$trxs, dat_perf$trys, 5, 5)))["elapsed"]

    fu_t <- tree.functional.grow(dat_perf$trxs, dat_perf$trys, 5, 5)
    im_t <- tree.grow(dat_perf$trxs, dat_perf$trys, 5, 5)

    ti_fu_pr <- system.time(replicate(nrep, tree.functional.classify(dat_perf$trxs, fu_t)))["elapsed"]
    ti_im_pr <- system.time(replicate(nrep, tree.classify(dat_perf$trxs, im_t)))["elapsed"]
} else {
    ti_fu_gr <- 0
    ti_im_gr <- 0
    ti_fu_pr <- 0
    ti_im_pr <- 0
}
@
<<perf-comp-plot,fig.keep='high',cache=FALSE,echo=FALSE>>=
barplot(c(ti_fu_gr, ti_im_gr, ti_fu_pr, ti_im_pr), names.arg = c("grow functional", "grow imperative", "predict functional", "predict imperative"))
@
\caption{The performance of the two implementations (less is better).}
\end{figure}

\subsection{Parameter search}
\label{subsec:params}

\begin{landscape}
<<par-search-comp,cache=TRUE,echo=FALSE>>=
nmins <- c(0,1,2,3,4,5,6,8,10,15,20,30,40,50,80,110,140,170,200,250,300,400,500,750,1000,1500,2000,3000)
minleafs <- c(0,1,2,3,4,5,6,8,10,15, 25, 50, 100, 200, 500)
ps_pars <- list()
for(i in 1:length(nmins)) {
    for(j in 1:length(minleafs)) {
        # The minleaf constraint implies all nmin constraints for which
        # nmins <= 2*minleaf holds. Just skip them.
        if(nmins[i] > 2 * minleafs[j]) {
            ps_pars[[length(ps_pars) + 1]] <- list(nmin = nmins[i], minleaf = minleafs[j])
        }
    }
}
# we are not using the labels...
ps_lbls <- rep(NA, length(ps_pars))
e_ps <- eval_mthd(dat_pars, ps_lbls, ps_pars, eval_with_pars)
@
<<par-search-tbl,cache=FALSE,results='asis',echo=FALSE>>=
etab <- eval_to_df(e_ps)
#ctab <- xtabs(error ~ nmin + minLeaf, data = etab)
ctab <- with(etab, tapply(error, list(nmin, minLeaf), sum))
digits <- c(0, rep(3, dim(ctab)[2]))
tab <- xtable(ctab, digits = digits)
print(tab, size = "\\footnotesize")
@
\captionof{table}{The results of the parameter search. The first row indicates the minleaf, the first column the nmin used.}
\end{landscape}
%\end{figure}

\begin{thebibliography}{1}

\bibitem{SPAM}
  Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt.
  \emph{SPAM E-mail Database}.
  Hewlett-Packard Labs, 1501 Page Mill Rd., 
  Palo Alto, CA 94304,
  June-July 1999.


\end{thebibliography}

\end{document}
