\documentclass[12pt, a4paper, oneside]{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{pdflscape}

% font size could be 10pt (default), 11pt or 12 pt
% paper size coulde be letterpaper (default), legalpaper, executivepaper,
% a4paper, a5paper or b5paper
% side coulde be oneside (default) or twoside 
% columns coulde be onecolumn (default) or twocolumn
% graphics coulde be final (default) or draft 
%
% titlepage coulde be notitlepage (default) or titlepage which 
% makes an extra page for title 
% 
% paper alignment coulde be portrait (default) or landscape 
%
% equations coulde be 
%   default number of the equation on the rigth and equation centered 
%   leqno number on the left and equation centered 
%   fleqn number on the rigth and  equation on the left side
%   
\title{MDM Assignment 1}
\author{Marco Vassena  \\
    4110161 \\
    \and 
    Philipp Hausmann \\
    4003373 \\
    }

\date{\today} 
\begin{document}

<<setup, echo=FALSE, cache=FALSE,include=FALSE>>=
source('../src/benchmark.r', chdir=TRUE)
library(knitr)
library(xtable)
@

\maketitle

\tableofcontents


\section{Problem description}
The goal of this assignment is to implement and evaluate a tree classification algorithm.
In addition to that, we will also discuss two different implementation styles
and their performance implications.

\section{Implementations}
We created two implementations of the algorithm. The first one is idiomatic R code,
whereas the second one is inspired by what a functional implementation looks like.
Both algorithms produce the same results, they may only differ performance-wise.

\subsection{Basic Functions}
The file \texttt{common.r} contains utility functions common to both the implementations. 
In this section we will point out only the peculiarities about some of those functions.

	\paragraph{\texttt{best.split.on}} 
	This function computes the best possible split, for a given vector of 
	attributes. Note that this function supports both numerical and binary 
	(encoded as 0 and 1) attributes. Splits on binary attributes have threshold
  value 0.5.
	Since the returned split must  also satisfy the \texttt{minleaf} constraint we 
	did not adopt the optimization of considering exclusively segment borders, 
	as they are not compatible.

	\paragraph{\texttt{majority\_vote}}
	This function computes the majority class prediction for a given vector 
	containing binary labels (0 and 1). Ties are broken at random.

\subsection{Imperative/R Style}
The classification tree has been implemented as a non-empty data frame with the following columns: \texttt{left}, \texttt{right}, \texttt{label}, \texttt{split}, \texttt{splitCol}. Each row represents either a leaf or an internal node and the first row is the root of the tree.
The \texttt{left} and \texttt{right} field of a node row contains the index of the row of the same data frame at which the correspondent left and right child is to be found.
The \texttt{split} and \texttt{splitCol} are used by the \texttt{classify} procedure. 
The former is the threshold value and the latter is the column number of the attribute referred by the first.
The field \texttt{label} is set to \texttt{NA} for node rows.
Leaves nodes fields are all set to \texttt{NA} except for \texttt{label} which contains the predicted class label (0 or 1), computed using majority vote in the \texttt{grow} procedure.

\subsection{Functional Style}
The functional style version encodes the tree by simulating objects in R, 
using \texttt{S3} class. We would like to note that R is not perfectly 
suited for this approach, but it should give some indications how such an 
approach compares in term of performance to a more procedural/imperative approach. 
The source code can be found in the file \texttt{functional.r}.

The implementation basically uses recursion to construct the tree, which happens
in the \texttt{tree.functional.grow} function. A nice side effect of the functional
approach is, that the tree is never modified and the recursive approach could easily
be parallelized (in general, not necessarily in R).

\section{Results}
\subsection{Data Set description}
We have tested our implementation with the data set 
\href{https://archive.ics.uci.edu/ml/datasets/Spambase}{SPAM E-mail Database} \cite{SPAM}.
The data set contains only continuos and binary attributes and no missing values, 
thus it's compatible with our implementation.
We have chosen this data set, because it has a great number of attributes (57)
thus it can be conviniently analyzed by classification trees, as they automatically
select relevant attributes.
The attributes include percentage of specific words or characters present in the emails 
and statistics (average, longest sequence, total length) about sequences of capital 
letters in the email. The data set include also a binary attribute that denotes 
whether an email is spam or not. The data set consists of 4601 instances.
For further information about the data set see \cite{SPAM}.


\subsection{Parameter search}
\label{subsec:params}
The script \texttt{benchmark.r} contains the function \texttt{search.params} which
trains a classification tree with different settings of the \texttt{nmin} and \texttt{minleaf} 
parameters using 70\% of our data set (randomly sampled). The corresponding error rate
is computed using the remaining 30\% as testing data set. 
Parameter combinations where the \texttt{minleaf} constraint is stronger
than the \texttt{nmin} constraint have been omitted, as the variation in \texttt{nmin} 
is irrelevant in that case.  The results can be seen in table \ref{tbl:par-res}. 
The best setting for the given dataset is a \texttt{minleaf} of 6
and a \texttt{nmin} of 15, which gives an error rate of $10.0\%$.

\begin{landscape}

<<par-search-comp,cache=TRUE,echo=FALSE>>=
e_ps <- search.params()
@

<<par-search-tbl,cache=FALSE,results='asis',echo=FALSE>>=
etab <- eval_to_df(e_ps)
#ctab <- xtabs(error ~ nmin + minLeaf, data = etab)
ctab <- with(etab, tapply(error, list(nmin, minLeaf), sum))
digits <- c(0, rep(3, dim(ctab)[2]))
tab <- xtable(ctab, digits = digits, caption = "The results of the parameter search. The first row indicates the minleaf, the first column the nmin used.", label = "tbl:par-res")
print(tab, size = "\\footnotesize")
@
\end{landscape}

\subsection{Performance comparison}
The script \texttt{benchmark} contains the function \texttt{benchmark.tree}, which compares the 
running time of the two implementations, using the library \texttt{microbenchmark}.
The benchmark has been performed on the whole Pima indians data set, on both \texttt{grow} and
\texttt{classify} functions. Each function has been run 5 times.
As can be seen in figure \ref{plot:perf}, growing times does not differ significantly,
the imperative version being about a decisecond slower. In prediction the imperative
implementation is slightly faster, taking about ten microseconds less than the functional
one.

\begin{figure}[!ht]
<<par-bench-comp,cache=TRUE,echo=FALSE>>=
bench <- benchmark.tree()
@
<<perf-bench-plot,fig.keep='high',cache=FALSE,echo=FALSE>>=
boxplot(bench$grow, names = c("grow functional", "grow imperative"),
        xlab = "", ylab = "log(time) [s]")
boxplot(bench$classify, names = c("predict functional", "predict imperative"), 
        xlab = "", ylab = expression(paste("log(time) [", ~ mu, "s]")))
@
\label{plot:perf}
\end{figure}

\begin{thebibliography}{1}

\bibitem{SPAM}
  Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt.
  \emph{SPAM E-mail Database}.
  Hewlett-Packard Labs, 1501 Page Mill Rd., 
  Palo Alto, CA 94304,
  June-July 1999.


\end{thebibliography}

\end{document}
